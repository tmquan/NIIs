{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cameras.ipynb\n",
      "\u001b[0m\u001b[01;36mdata\u001b[0m@\n",
      "DataAndEnvironment.ipynb\n",
      "data.py\n",
      "model.py\n",
      "nerp.py\n",
      "nerv.py\n",
      "\u001b[01;34m__pycache__\u001b[0m/\n",
      "README.md\n",
      "test_camera_0_features_-0.0108_0.3133_-0.7449_0.0179_0.2330_-0.9397.png\n",
      "test_camera_1_features_-0.2786_-0.6991_0.8099_0.4372_-0.0442_-0.0797.png\n",
      "test_camera_2_features_0.9018_-0.8738_-0.6735_0.9631_0.6722_0.7186.png\n",
      "test_camera_3_features_0.6825_0.3112_0.9095_0.5384_-0.2164_-0.5489.png\n",
      "test_camera_4_features_-0.3895_0.5603_0.8177_0.4621_-0.3504_-0.9236.png\n",
      "test_camera_5_features_-0.3143_-0.8350_0.9672_0.2261_0.4855_-0.6276.png\n",
      "test_camera_6_features_0.2507_0.2731_0.1764_-0.3692_-0.8947_0.8825.png\n",
      "test_camera_7_features_0.0704_0.8853_0.8612_0.9380_0.9240_-0.7961.png\n",
      "\u001b[01;34mtest_data_full_01\u001b[0m/\n",
      "\u001b[01;34mtest_data_window_-200_1500\u001b[0m/\n",
      "\u001b[01;34mtest_data_window_-500_3071\u001b[0m/\n",
      "VolRen.ipynb\n",
      "1593\n",
      "['data/ChestXRLungSegmentation/Imagenglab/processed/train/images/001.nii.gz']\n",
      "23389\n",
      "['data/ChestXRLungSegmentation/VinDr/v1/processed/train/images/000434271f63a053c4128a0ba6352c7f.png']\n",
      "1593\n",
      "['data/ChestXRLungSegmentation/Imagenglab/processed/train/images/001.nii.gz']\n",
      "5077\n",
      "['data/ChestXRLungSegmentation/VinDr/v1/processed/test/images/002a34c58c5b758217ed1f584ccbcfe9.png']\n",
      "1593\n",
      "['data/ChestXRLungSegmentation/Imagenglab/processed/train/images/001.nii.gz']\n",
      "5077\n",
      "['data/ChestXRLungSegmentation/VinDr/v1/processed/test/images/002a34c58c5b758217ed1f584ccbcfe9.png']\n"
     ]
    }
   ],
   "source": [
    "%ls\n",
    "%run DataAndEnvironment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# cameras = RandomCameras(batch_size=hparams.batch_size, random=True).to(device)\n",
    "\n",
    "# render_size describes the size of both sides of the \n",
    "# rendered images in pixels. We set this to the same size\n",
    "# as the target images. I.e. we render at the same\n",
    "# size as the ground truth images.\n",
    "render_size = hparams.shape\n",
    "\n",
    "# Our rendered scene is centered around (0,0,0) \n",
    "# and is enclosed inside a bounding box\n",
    "# whose side is roughly equal to 3.0 (world units).\n",
    "volume_extent_world = 4.0\n",
    "\n",
    "# 1) Instantiate the raysampler.\n",
    "# Here, NDCMultinomialRaysampler generates a rectangular image\n",
    "# grid of rays whose coordinates follow the PyTorch3D\n",
    "# coordinate conventions.\n",
    "# Since we use a volume of size 128^3, we sample n_pts_per_ray=150,\n",
    "# which roughly corresponds to a one ray-point per voxel.\n",
    "# We further set the min_depth=0.1 since there is no surface within\n",
    "# 0.1 units of any camera plane.\n",
    "raysampler = NDCMultinomialRaysampler(\n",
    "    image_width=render_size,\n",
    "    image_height=render_size,\n",
    "    n_pts_per_ray=512,\n",
    "    min_depth=0.001,\n",
    "    max_depth=volume_extent_world,\n",
    ")\n",
    "\n",
    "\n",
    "# 2) Instantiate the raymarcher.\n",
    "# Here, we use the standard EmissionAbsorptionRaymarcher \n",
    "# which marches along each ray in order to render\n",
    "# each ray into a single 3D color vector \n",
    "# and an opacity scalar.\n",
    "raymarcher = EmissionAbsorptionRaymarcher()\n",
    "\n",
    "# Finally, instantiate the volumetric render\n",
    "# with the raysampler and raymarcher objects.\n",
    "renderer = VolumeRenderer(\n",
    "    raysampler=raysampler, \n",
    "    raymarcher=raymarcher,\n",
    ")\n",
    "\n",
    "# Instantiate the volumetric model.\n",
    "# We use a cubical volume with the size of \n",
    "# one side = 128. The size of each voxel of the volume \n",
    "# is set to volume_extent_world / volume_size s.t. the\n",
    "# volume represents the space enclosed in a 3D bounding box\n",
    "# centered at (0, 0, 0) with the size of each side equal to 3.\n",
    "volume_size = hparams.shape\n",
    "volume_model = VolumeModel(\n",
    "    renderer,\n",
    "    volume_size = [volume_size] * 3, \n",
    "    voxel_size = volume_extent_world / volume_size,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "debug_data = first(datamodule.train_dataloader())\n",
    "image3d = debug_data['image3d'].to(device)\n",
    "volumes = Volumes(\n",
    "    features = torch.cat([image3d]*3, dim=1),\n",
    "    densities = torch.ones_like(image3d) / 1000., \n",
    "    voxel_size = volume_extent_world / volume_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = NeuralCameras(\n",
    "    batch_size=hparams.batch_size,\n",
    "    random = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cameras = RandomCameras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cameras.get_camera_center()[0]) # xyz world coordinates\n",
    "# print(cameras.get_projection_transform().get_matrix()[0]) # projection matrix\n",
    "# print(cameras.get_world_to_view_transform().get_matrix()[0]) # world to view transform\n",
    "# print(cameras.get_full_projection_transform().get_matrix()[0]) # world to view transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0000e+00,  2.7380e-06, -2.1298e-03],\n",
      "        [ 0.0000e+00,  1.0000e+00,  1.2856e-03],\n",
      "        [ 2.1298e-03,  1.2856e-03, -1.0000e+00]])\n",
      "tensor([4.6566e-10, 4.6566e-10, 3.2272e+00])\n",
      "tensor([[42.5664]])\n",
      "tensor([[1.2271]])\n",
      "tensor([[-1.0000e+00,  2.7380e-06, -2.1298e-03],\n",
      "        [ 0.0000e+00,  1.0000e+00,  1.2856e-03],\n",
      "        [ 2.1298e-03,  1.2856e-03, -1.0000e+00]])\n",
      "tensor([4.6566e-10, 4.6566e-10, 3.2272e+00])\n",
      "tensor([[42.5664]])\n",
      "tensor([[1.2271]])\n",
      "tensor([[-1.0000e+00,  2.7380e-06, -2.1298e-03],\n",
      "        [ 0.0000e+00,  1.0000e+00,  1.2856e-03],\n",
      "        [ 2.1298e-03,  1.2856e-03, -1.0000e+00]])\n",
      "tensor([4.6566e-10, 4.6566e-10, 3.2272e+00])\n",
      "tensor([[42.5664]])\n",
      "tensor([[1.2271]])\n",
      "tensor([[-1.0000e+00,  2.7380e-06, -2.1298e-03],\n",
      "        [ 0.0000e+00,  1.0000e+00,  1.2856e-03],\n",
      "        [ 2.1298e-03,  1.2856e-03, -1.0000e+00]])\n",
      "tensor([4.6566e-10, 4.6566e-10, 3.2272e+00])\n",
      "tensor([[42.5664]])\n",
      "tensor([[1.2271]])\n"
     ]
    }
   ],
   "source": [
    "for idx in range(hparams.batch_size):\n",
    "    print(cameras.R[idx]) # rotation matrix\n",
    "    print(cameras.T[idx]) # translation vector\n",
    "    print(cameras.fov[idx]) # field of view\n",
    "    print(cameras.aspect_ratio[idx]) # aspect ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd0ba7896d619d2e23297155645cc474a0b2bf660f5d50e2d5184ff5ae24a4e1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
